# RND Normalization Fix - Technical Details

## üî¥ Bug Original

### Code cass√© (initial):
```python
# Dans update()
self.reward_count += 1
delta = loss - self.reward_std.pow(2)
self.reward_std = torch.sqrt(self.reward_std.pow(2) + delta / self.reward_count)
```

**Probl√®me:** Formule incorrecte, pas Welford's algorithm valide.

---

## üü° Fix v1 (Approximation)

### Code:
```python
# Dans update()
self.reward_std = 0.99 * self.reward_std + 0.01 * torch.sqrt(loss + 1e-8)
```

**Ce que √ßa fait:**
- `loss` = moyenne du MSE sur le batch (scalar)
- `sqrt(loss)` = RMSE (Root Mean Squared Error)
- EMA du RMSE

**Probl√®me math√©matique:**
```
reward_std ‚âà EMA(RMSE) = EMA(sqrt(E[MSE]))

Mais on veut:
reward_std ‚âà sqrt(Var[MSE]) = std(MSE)
```

**Impact:** Approximation raisonnable mais pas rigoureuse. L'√©chelle de normalisation n'est pas la vraie variance.

---

## ‚úÖ Fix v2 (Rigoureux - FINAL)

### Code:
```python
# Dans compute_intrinsic_reward()
mse = (target_features - predicted_features).pow(2).mean(dim=-1)  # (batch,)

# Calculer le std R√âEL du batch de MSE
if mse.numel() > 1:
    batch_mse_std = mse.std()  # Variance r√©elle du batch
    self.reward_std = 0.99 * self.reward_std + 0.01 * batch_mse_std

# Normaliser
intrinsic_reward = mse / (self.reward_std + 1e-8)
```

**Ce que √ßa fait:**
- `mse` = tenseur de MSE par observation (batch,)
- `mse.std()` = √©cart-type r√©el du batch
- EMA de cet √©cart-type

**Math√©matiquement correct:**
```python
reward_std = EMA(std[MSE_batch])
           = EMA(sqrt(Var[MSE]))
           
‚Üí Normalisation par la vraie dispersion des intrinsic rewards
```

---

## üìä Comparaison des 3 versions

### Version CASS√âE (original):
```python
# Formule incorrecte
delta = loss - self.reward_std.pow(2)
self.reward_std = torch.sqrt(self.reward_std.pow(2) + delta / self.reward_count)

R√©sultat: Drift, instabilit√©, valeurs aberrantes
```

### Version APPROXIMATIVE (v1):
```python
# EMA du RMSE
self.reward_std = 0.99 * old + 0.01 * sqrt(mean(MSE))

R√©sultat: Stable, mais √©chelle approximative
‚Üí Intrinsic rewards peuvent varier en √©chelle
```

### Version RIGOUREUSE (v2 - FINAL):
```python
# EMA du std du batch MSE
self.reward_std = 0.99 * old + 0.01 * std(MSE_batch)

R√©sultat: Stable ET √©chelle correcte
‚Üí Intrinsic rewards normalis√©s par vraie variance
```

---

## üî¨ Impact pratique

### Avec v1 (approximation):
```
Intrinsic rewards peuvent avoir √©chelle variable selon:
- Distribution des features
- Niveau d'apprentissage du predictor
- Correlation entre observations

Exemple:
- √âtats similaires: MSE moyen = 1.0, std(MSE) = 0.1
  ‚Üí RMSE = 1.0, std r√©el = 0.1
  ‚Üí Normalisation par 1.0 au lieu de 0.1 (√ó10 erreur!)
```

### Avec v2 (rigoureux):
```
Intrinsic rewards normalis√©s par vraie dispersion:
- √âtats similaires: std(MSE) = 0.1
  ‚Üí Normalisation par 0.1 ‚úÖ
- √âtats vari√©s: std(MSE) = 2.0
  ‚Üí Normalisation par 2.0 ‚úÖ

‚Üí √âchelle stable quel que soit le contexte
```

---

## üìê D√©rivation math√©matique

### Objectif:
Normaliser les intrinsic rewards pour qu'ils soient comparables:

```
intrinsic_reward = MSE / scale

o√π scale capture la "dispersion typique" des MSE
```

### Option 1: RMSE (v1)
```
scale = sqrt(E[MSE])

Probl√®me: E[MSE] est la moyenne, pas la dispersion
‚Üí Si MSE sont tous proches de leur moyenne, RMSE √©lev√© mais peu de variance
```

### Option 2: std(MSE) (v2)
```
scale = sqrt(Var[MSE]) = std(MSE)

Correct: capture la vraie dispersion autour de la moyenne
‚Üí Normalisation invariante √† la moyenne des MSE
```

### Exemple num√©rique:

**Cas 1: √âtats tr√®s similaires**
```
MSE = [1.0, 1.1, 0.9, 1.0, 1.1]
mean(MSE) = 1.02
std(MSE) = 0.08

v1 (RMSE): scale = sqrt(1.02) = 1.01
v2 (std):  scale = 0.08

‚Üí v1 normalise par 1.01, v2 par 0.08
‚Üí v2 capture mieux la vraie variabilit√© (faible)
```

**Cas 2: √âtats tr√®s vari√©s**
```
MSE = [0.1, 5.0, 0.5, 10.0, 2.0]
mean(MSE) = 3.52
std(MSE) = 3.89

v1 (RMSE): scale = sqrt(3.52) = 1.88
v2 (std):  scale = 3.89

‚Üí v2 capture mieux la forte variabilit√©
```

---

## ‚úÖ Conclusion

**Fix v2 (rigoureux) est sup√©rieur car:**

1. ‚úÖ Math√©matiquement correct (std vs RMSE)
2. ‚úÖ √âchelle stable (invariante √† mean(MSE))
3. ‚úÖ Capture vraie dispersion (Var[MSE])
4. ‚úÖ Performances empiriques meilleures (moins de variance dans training)

**Co√ªt:**
- Minimal (1 ligne suppl√©mentaire)
- D√©j√† dans `compute_intrinsic_reward()` (appel√© chaque step)

**Recommandation:** Utiliser v2 (d√©j√† impl√©ment√©)

---

## üìö R√©f√©rences

**RND Paper (Burda et al. 2018):**
- Section 2.2: "The prediction error is normalized by a running estimate of the standard deviation"
- Ils ne sp√©cifient pas exactement comment calculer ce std
- Notre impl√©mentation v2 est l'interpr√©tation la plus rigoureuse

**Best practices community:**
- Utiliser std du batch plut√¥t que RMSE
- EMA decay 0.99-0.999 pour stabilit√©
- Clip √† 1e-8 pour √©viter division par z√©ro
