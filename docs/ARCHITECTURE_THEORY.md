# Architecture & Th√©orie du Projet - Deep Reinforcement Learning pour Football

## üìö Table des mati√®res
1. [Agents (PPO + Policy)](#agents)
2. [Environnements (Wrappers & Rewards)](#environnements)
3. [Mod√®les (R√©seaux de neurones)](#mod√®les)
4. [Training (Entra√Ænement)](#training)
5. [Evaluation (M√©triques)](#evaluation)

---

## 1. Agents (PPO + Policy) ü§ñ

### `agents/ppo.py` - Proximal Policy Optimization

#### **Th√©orie: Qu'est-ce que PPO?**

PPO est un algorithme d'apprentissage par renforcement **on-policy** qui optimise une politique (policy) en maximisant les r√©compenses tout en restant proche de l'ancienne politique.

**Probl√®me r√©solu:**
- Les algorithmes policy gradient classiques (REINFORCE) sont instables: un mauvais gradient peut d√©truire la politique
- PPO limite les changements de politique √† chaque update pour garantir la stabilit√©

**√âquation cl√© - Objectif PPO:**
```
L^CLIP(Œ∏) = E_t[min(r_t(Œ∏) * A_t, clip(r_t(Œ∏), 1-Œµ, 1+Œµ) * A_t)]

O√π:
- r_t(Œ∏) = œÄ_Œ∏(a_t|s_t) / œÄ_Œ∏_old(a_t|s_t)  (ratio de probabilit√©s)
- A_t = advantage (√† quel point l'action √©tait meilleure que la moyenne)
- Œµ = clip_epsilon (typiquement 0.2)
```

**Intuition:**
- Si l'action √©tait bonne (A_t > 0), on augmente sa probabilit√©
- Mais on limite l'augmentation avec le clipping pour √©viter les changements trop brutaux
- Si l'action √©tait mauvaise (A_t < 0), on diminue sa probabilit√© (avec clipping aussi)

**Composants dans le code:**

1. **Policy Loss** (ligne ~140):
   ```python
   ratio = exp(new_log_prob - old_log_prob)  # Ratio de probabilit√©s
   surr1 = ratio * advantages                 # Objectif non clipp√©
   surr2 = clip(ratio, 1-Œµ, 1+Œµ) * advantages # Objectif clipp√©
   policy_loss = -min(surr1, surr2)           # On prend le min (pessimiste)
   ```

2. **Value Loss** (ligne ~143):
   ```python
   value_loss = (returns - values)^2  # Erreur quadratique
   ```
   - Le critique (value function) estime V(s) = r√©compense future attendue
   - On minimise l'erreur entre la pr√©diction et le return r√©el

3. **Entropy Bonus** (ligne ~152):
   ```python
   entropy_loss = -entropy  # On maximise l'entropie
   ```
   - Encourage l'exploration en p√©nalisant les politiques trop d√©terministes
   - Diminue progressivement (annealing) pour converger vers une politique d√©terministe

**Hyperparam√®tres importants:**
- `clip_epsilon = 0.2`: Limite les changements de politique (¬±20%)
- `value_coef = 0.5`: Poids de la value loss
- `entropy_coef = 0.01‚Üí0.002`: Poids de l'entropie (d√©cro√Æt avec le temps)
- `ppo_epochs = 4`: Nombre de passes sur les donn√©es collect√©es

---

### `agents/policy.py` - R√©seau de politique LSTM

#### **Th√©orie: Pourquoi LSTM?**

Le football n√©cessite de la **m√©moire temporelle**:
- Se souvenir o√π sont les joueurs
- Anticiper les trajectoires
- Planifier des s√©quences d'actions (dribble, passe, tir)

**Architecture:**
```
Observation (161 dim)
    ‚Üì
MLP Encoder (512 hidden units)
    ‚Üì
LSTM (128 hidden, 1 layer)
    ‚Üì
    ‚îú‚Üí Actor Head ‚Üí Distribution d'actions (19 actions)
    ‚îî‚Üí Critic Head ‚Üí Value V(s)
```

**Composants:**

1. **Encoder MLP** (ligne ~50):
   ```python
   obs ‚Üí Linear(512) ‚Üí ReLU ‚Üí Linear(512) ‚Üí ReLU
   ```
   - Extrait des features de l'observation brute
   - 512 dimensions pour capturer la complexit√© du jeu

2. **LSTM** (ligne ~60):
   ```python
   features ‚Üí LSTM(128) ‚Üí hidden_state
   ```
   - **Hidden state (h_t)**: M√©moire √† court terme (position actuelle)
   - **Cell state (c_t)**: M√©moire √† long terme (contexte du match)
   - Permet de se souvenir des 10-20 derniers steps

3. **Actor (Policy)** (ligne ~70):
   ```python
   hidden ‚Üí Linear(action_dim) ‚Üí Categorical distribution
   ```
   - Sortie: Probabilit√©s pour chaque action (19 actions possibles)
   - Exemple: [0.1, 0.05, 0.3, ...] ‚Üí 30% de chance de tirer

4. **Critic (Value)** (ligne ~80):
   ```python
   hidden ‚Üí Linear(1) ‚Üí V(s)
   ```
   - Sortie: Valeur de l'√©tat (r√©compense future attendue)
   - Utilis√© pour calculer les advantages

**Pourquoi s√©parer Actor et Critic?**
- **Actor-Critic architecture**: Deux t√™tes qui partagent le m√™me encoder
- Actor apprend QUOI faire (actions)
- Critic apprend COMBIEN √ßa vaut (valeur)
- Plus stable que d'avoir un seul r√©seau

---

## 2. Environnements (Wrappers & Rewards) üèüÔ∏è

### `envs/wrappers.py` - Wrappers d'environnement

#### **Th√©orie: Pourquoi des wrappers?**

Google Research Football (GRF) retourne des observations brutes complexes. Les wrappers:
1. Simplifient les observations
2. Normalisent les donn√©es
3. Ajoutent du reward shaping

**Wrappers principaux:**

#### 1. **GRFtoGymnasiumWrapper** (ligne ~50)
```python
Objectif: Convertir GRF (ancien Gym) vers Gymnasium (nouveau standard)
```

**Probl√®me r√©solu:**
- GRF utilise l'ancienne API Gym (step retourne 4 valeurs)
- Gymnasium utilise la nouvelle API (step retourne 5 valeurs)
- Conversion: `(obs, reward, done, info)` ‚Üí `(obs, reward, terminated, truncated, info)`

**Fonctions cl√©s:**
- `step()`: Convertit le format de sortie
- `get_original_obs()`: R√©cup√®re l'observation brute (pour stats)
- `get_current_score()`: Extrait le score du match

#### 2. **ObsWrapperRaw** (ligne ~150)
```python
Objectif: Encoder les observations brutes en vecteur num√©rique
```

**Transformation:**
```
Observation GRF (dict avec ~30 cl√©s)
    ‚Üì
Vecteur num√©rique (161 dimensions)
```

**Exemple d'encodage:**
- Positions des joueurs: `left_team` (11 joueurs √ó 2 coords = 22 dims)
- Position du ballon: `ball` (3 coords)
- Direction du ballon: `ball_direction` (3 coords)
- Joueur actif: `active` (1 dim)
- Sticky actions: `sticky_actions` (10 dims)
- Score: `score` (2 dims)
- etc.

**Pourquoi 161 dimensions?**
- Somme de toutes les features pertinentes
- Assez compact pour le r√©seau de neurones
- Contient toute l'information n√©cessaire

#### 3. **RunningNormWrapper** (ligne ~250)
```python
Objectif: Normaliser les observations pour stabiliser l'apprentissage
```

**Th√©orie: Pourquoi normaliser?**
- Les r√©seaux de neurones apprennent mieux avec des donn√©es normalis√©es (moyenne=0, std=1)
- Sans normalisation: certaines features dominent (ex: positions en [-1, 1] vs vitesses en [-0.1, 0.1])

**M√©thode:**
```python
obs_normalized = (obs - running_mean) / (running_std + epsilon)
```

**Running statistics:**
- Moyenne et √©cart-type calcul√©s **en ligne** (pendant le training)
- Mis √† jour √† chaque step avec une moyenne mobile
- Sauvegard√©s dans les checkpoints pour l'√©valuation

**‚ö†Ô∏è Attention:**
- D√©sactiv√© par d√©faut (`running_norm: false`) car peut causer des incoh√©rences train/eval
- Si activ√©, il faut utiliser les m√™mes stats en eval

---

### `envs/make_env.py` - Cr√©ation d'environnements

#### **Th√©orie: Environnements vectoris√©s**

**Probl√®me:**
- Collecter des donn√©es avec 1 seul env est lent
- Solution: Parall√©liser avec N environnements

**AsyncVectorEnv:**
```python
24 environnements en parall√®le
    ‚Üì
24 workers (processus s√©par√©s)
    ‚Üì
Collecte 24√ó plus de donn√©es par seconde
```

**Avantages:**
1. **Vitesse**: 24√ó plus rapide
2. **Diversit√©**: Exp√©riences vari√©es (diff√©rents √©tats de jeu)
3. **Stabilit√©**: Moyennes sur plusieurs envs ‚Üí gradients plus stables

**Fonctionnement:**
```python
obs = envs.reset()  # Reset 24 envs ‚Üí (24, 161)
actions = policy(obs)  # 24 actions
next_obs, rewards, dones = envs.step(actions)  # Step parall√®le
```

---

### `envs/rewards.py` - Reward Shaping

#### **Th√©orie: Pourquoi du reward shaping?**

**Probl√®me des r√©compenses sparses:**
- GRF donne +1 pour un but, -1 pour un but encaiss√©
- Un but arrive tous les ~100-500 steps
- L'agent ne re√ßoit presque jamais de feedback ‚Üí n'apprend rien

**Solution: Reward Shaping**
```
Reward total = Reward GRF (sparse) + Reward shaping (dense)
```

**R√©compenses denses (rewards_dense.yaml):**

1. **Ball distance** (+0.005):
   ```python
   reward = -distance_to_ball * 0.005
   ```
   - Encourage l'agent √† aller vers le ballon
   - Feedback constant

2. **Goal distance** (+0.02):
   ```python
   reward = -(distance_to_goal - prev_distance_to_goal) * 0.02
   ```
   - R√©compense pour se rapprocher du but adverse
   - P√©nalit√© pour s'en √©loigner

3. **Shot attempt** (+5.0):
   ```python
   if action == SHOT and ball_x > 0.85 and ball_direction_x > 0:
       reward = +5.0
   ```
   - √âNORME r√©compense pour tirer (proche du but ET vers le but)
   - Force l'exploration du tir

4. **Pass** (+0.1):
   ```python
   if ball_owner_changed and same_team and forward_pass:
       reward = +0.1
   ```
   - Encourage les passes vers l'avant

**Annealing (d√©croissance):**
```python
reward_shaped = reward_shaped * annealing_factor
annealing_factor: 1.0 ‚Üí 0.0 (sur 10M steps)
```
- Au d√©but: Beaucoup de shaping (guide l'agent)
- √Ä la fin: Seulement les vrais buts comptent (performance r√©elle)

---

## 3. Mod√®les (R√©seaux de neurones) üß†

### `models/policy_lstm.py` - Architecture de la politique

Voir section [Agents/Policy](#agents-policy---r√©seau-de-politique-lstm) ci-dessus.

---

### `models/encoders.py` - Encodeurs d'observations

#### **Th√©orie: Pourquoi un encodeur?**

**Probl√®me:**
- Observation GRF = dict complexe avec ~30 cl√©s
- R√©seau de neurones = besoin d'un vecteur num√©rique fixe

**Solution: Encoder**
```
Dict GRF ‚Üí Vecteur (161 dims) ‚Üí R√©seau de neurones
```

**Types d'encodeurs:**

#### 1. **RawObsEncoder** (Simple115 style)
```python
Concat√®ne toutes les features importantes:
- Positions des joueurs (22 dims)
- Position du ballon (3 dims)
- Vitesses (22 dims)
- Sticky actions (10 dims)
- Game mode (7 dims one-hot)
- etc.
Total: 161 dimensions
```

**Avantages:**
- Simple et efficace
- Toute l'information est pr√©serv√©e
- Fonctionne bien avec LSTM

#### 2. **SMM Encoder** (Spatial Mini-Map)
```python
Convertit les positions en image 2D:
- Terrain divis√© en grille (ex: 96√ó72)
- Chaque joueur = pixel
- Canaux: [joueurs alli√©s, joueurs adverses, ballon]
```

**Avantages:**
- Repr√©sentation spatiale naturelle
- Peut utiliser des CNN
- Bon pour la vision globale du jeu

**Dans ce projet:**
- On utilise **RawObsEncoder** (plus simple et efficace)
- SMM est plus adapt√© pour des approches visuelles

---

## 4. Training (Entra√Ænement) üéì

### `training/trainer.py` - Boucle d'entra√Ænement principale

#### **Th√©orie: Comment entra√Æner un agent RL?**

**Algorithme g√©n√©ral (PPO):**
```
1. Collecter des exp√©riences (rollouts)
2. Calculer les advantages
3. Optimiser la politique avec PPO
4. R√©p√©ter
```

**D√©tails de chaque √©tape:**

#### **√âtape 1: Collecte de rollouts** (ligne ~450)
```python
for step in range(rollout_len):  # 256 steps
    action = policy(obs, lstm_state)
    next_obs, reward, done = env.step(action)
    storage.add(obs, action, reward, value, log_prob)
```

**Qu'est-ce qu'un rollout?**
- S√©quence d'exp√©riences: (s_0, a_0, r_0, s_1, a_1, r_1, ...)
- Longueur: 256 steps
- 24 environnements en parall√®le ‚Üí 6144 transitions par rollout

**Pourquoi 256 steps?**
- Compromis entre:
  - Court: Updates fr√©quents (plus stable)
  - Long: Plus de contexte pour LSTM (meilleure m√©moire)
- 256 = ~10-20 secondes de jeu

#### **√âtape 2: Calcul des advantages** (ligne ~620)
```python
advantages = GAE(rewards, values, next_value)
```

**Th√©orie: Generalized Advantage Estimation (GAE)**

**Probl√®me:**
- Comment savoir si une action √©tait bonne?
- Comparer avec quoi?

**Solution: Advantage**
```
A(s,a) = Q(s,a) - V(s)
       = "Valeur de l'action" - "Valeur moyenne de l'√©tat"
```

**Intuition:**
- A > 0: Action meilleure que la moyenne ‚Üí augmenter sa probabilit√©
- A < 0: Action pire que la moyenne ‚Üí diminuer sa probabilit√©

**GAE Formula:**
```
A_t = Œ¥_t + (Œ≥Œª)Œ¥_{t+1} + (Œ≥Œª)¬≤Œ¥_{t+2} + ...

O√π:
- Œ¥_t = r_t + Œ≥V(s_{t+1}) - V(s_t)  (TD error)
- Œ≥ = 0.993 (discount factor)
- Œª = 0.95 (GAE parameter)
```

**Pourquoi GAE?**
- Compromis entre:
  - Œª=0: Faible variance, biais √©lev√© (TD)
  - Œª=1: Variance √©lev√©e, pas de biais (Monte Carlo)
- Œª=0.95: Bon √©quilibre

#### **√âtape 3: Optimisation PPO** (ligne ~630)
```python
for epoch in range(4):  # 4 epochs
    for minibatch in get_batches(data, minibatch_size=3072):
        loss = ppo_loss(minibatch)
        loss.backward()
        optimizer.step()
```

**D√©tails:**
- **4 epochs**: On passe 4 fois sur les m√™mes donn√©es
- **Mini-batches**: Divise les 6144 transitions en batches de 3072
- **Gradient clipping**: Limite les gradients √† 0.5 (stabilit√©)

**Pourquoi plusieurs epochs?**
- R√©utiliser les donn√©es collect√©es (sample efficiency)
- Mais pas trop (sinon overfitting sur les vieilles donn√©es)

---

### `training/storage.py` - Rollout Buffer

#### **Th√©orie: Pourquoi un buffer?**

**Probl√®me:**
- On collecte 6144 transitions
- On doit les stocker quelque part
- On doit pouvoir les √©chantillonner en mini-batches

**Solution: RolloutStorage**

**Structure:**
```python
storage = {
    'obs': Tensor(256, 24, 161),      # Observations
    'actions': Tensor(256, 24),        # Actions
    'rewards': Tensor(256, 24),        # Rewards
    'values': Tensor(256, 24),         # Values V(s)
    'log_probs': Tensor(256, 24),      # Log probabilities
    'lstm_h': Tensor(256, 24, 128),    # LSTM hidden states
    'lstm_c': Tensor(256, 24, 128),    # LSTM cell states
    'returns': Tensor(256, 24),        # Computed returns
    'advantages': Tensor(256, 24),     # Computed advantages
}
```

**Fonctions cl√©s:**

1. **add()** (ligne ~100):
   ```python
   storage.add(step, obs, action, reward, value, log_prob, lstm_state)
   ```
   - Ajoute une transition au buffer
   - Appel√© √† chaque step de la collecte

2. **compute_returns()** (ligne ~150):
   ```python
   returns, advantages = compute_gae(rewards, values, next_value)
   ```
   - Calcule les returns et advantages avec GAE
   - Appel√© apr√®s la collecte, avant l'optimisation

3. **get_generator()** (ligne ~200):
   ```python
   for batch in storage.get_generator(batch_size, minibatch_size):
       yield batch
   ```
   - G√©n√®re des mini-batches pour l'optimisation
   - Shuffle les donn√©es pour √©viter l'overfitting

**Pourquoi stocker les LSTM states?**
- PPO a besoin de r√©√©valuer les actions avec la nouvelle politique
- Il faut le m√™me contexte LSTM que lors de la collecte
- Sinon les probabilit√©s seraient fausses

---

### `training/curriculum.py` - Curriculum Learning

#### **Th√©orie: Qu'est-ce que le curriculum learning?**

**Probl√®me:**
- Apprendre 11v11 directement est trop difficile
- L'agent ne marque jamais de but ‚Üí n'apprend rien

**Solution: Curriculum**
```
Facile ‚Üí Moyen ‚Üí Difficile
```

**Exemple de curriculum:**
```
Phase 1: academy_empty_goal_close (10M steps)
    ‚Üì (si winrate > 80%)
Phase 2: academy_empty_goal (10M steps)
    ‚Üì (si winrate > 70%)
Phase 3: academy_run_to_score (15M steps)
    ‚Üì (si winrate > 60%)
Phase 4: academy_3_vs_1_with_keeper (20M steps)
    ‚Üì (si winrate > 40%)
Phase 5: 11_vs_11_easy_stochastic (50M steps)
```

**Crit√®res d'avancement:**
- **Winrate**: % de victoires sur les 100 derniers √©pisodes
- **Min steps**: Nombre minimum de steps avant de pouvoir avancer
- **Max steps**: Nombre maximum de steps (force l'avancement)

**Avantages:**
1. **Apprentissage progressif**: Ma√Ætrise les bases avant les situations complexes
2. **Sample efficiency**: Apprend plus vite qu'en 11v11 direct
3. **Transfert**: Les comp√©tences apprises se transf√®rent aux phases suivantes

**Impl√©mentation:**
```python
if should_advance_curriculum():
    # Fermer les anciens envs
    envs.close()
    
    # Cr√©er les nouveaux envs (phase suivante)
    envs = create_vec_envs(new_env_name)
    
    # Reset LSTM states
    lstm_state = reset_lstm()
    
    # Continuer l'entra√Ænement
```

---

## 5. Evaluation (M√©triques) üìä

### `evaluation/evaluator.py` - √âvaluation de l'agent

#### **Th√©orie: Pourquoi √©valuer?**

**Probl√®me:**
- Pendant le training, l'agent explore (actions al√©atoires)
- On veut savoir sa vraie performance (sans exploration)

**Solution: √âvaluation p√©riodique**
```python
Tous les 25000 steps:
    1. Freeze la politique (mode d√©terministe)
    2. Jouer N √©pisodes (ex: 10)
    3. Calculer les m√©triques (winrate, goals, etc.)
    4. Logger dans TensorBoard
```

**Mode d√©terministe:**
```python
# Training (stochastique)
action = sample(policy(obs))  # Tire une action selon les probas

# Eval (d√©terministe)
action = argmax(policy(obs))  # Prend l'action la plus probable
```

**M√©triques √©valu√©es:**
- **Winrate**: % de victoires
- **Goals scored/conceded**: Buts marqu√©s/encaiss√©s
- **Episode length**: Dur√©e moyenne des matchs
- **Return**: R√©compense totale moyenne

---

### `evaluation/metrics.py` - Football Stats Tracker

#### **Th√©orie: M√©triques de football avanc√©es**

**Objectif:**
- Comprendre COMMENT l'agent joue
- Pas seulement s'il gagne, mais COMMENT il gagne

**M√©triques track√©es:**

#### 1. **Possession** (% du temps avec le ballon)
```python
possession = steps_with_ball / total_steps * 100
```
- Indicateur de domination
- Bon agent: 60-70% de possession

#### 2. **Tirs** (shots per game)
```python
shots = count(action == SHOT)
shots_on_target = count(action == SHOT and ball_direction_x > 0)
shot_accuracy = shots_on_target / shots * 100
```
- Indicateur d'agressivit√©
- Bon agent: 5-10 tirs/match, 50-70% cadr√©s

#### 3. **Passes** (passes per game)
```python
passes = count(ball_owner_changed and same_team)
pass_accuracy = passes_completed / passes_attempted * 100
```
- Indicateur de jeu collectif
- Bon agent: 70-80% de passes r√©ussies

#### 4. **Zones de jeu** (% du temps dans chaque zone)
```python
if x < -0.33: defense
elif x < 0.33: midfield
else: attack
```
- Indicateur de positionnement
- Bon agent: 20% d√©fense, 30% milieu, 50% attaque

#### 5. **Heatmaps** (positions cumul√©es)
```python
heatmap[x, y] += 1  # √Ä chaque step
```
- Visualisation des zones fr√©quent√©es
- Permet de voir les patterns de jeu

**Logging dans TensorBoard:**
```python
writer.add_scalar("football/possession_pct", possession, step)
writer.add_scalar("football/shots_per_game", shots, step)
writer.add_image("heatmap/player", heatmap, step)
```

---

## üéØ R√©sum√©: Comment tout fonctionne ensemble

```
1. ENVIRONNEMENT (envs/)
   ‚Üì
   Observations brutes (dict)
   ‚Üì
2. ENCODEUR (models/encoders.py)
   ‚Üì
   Vecteur num√©rique (161 dims)
   ‚Üì
3. POLITIQUE (models/policy_lstm.py)
   ‚Üì
   Action + Value + LSTM state
   ‚Üì
4. ENVIRONNEMENT
   ‚Üì
   Reward + Next obs
   ‚Üì
5. STORAGE (training/storage.py)
   ‚Üì
   Buffer de 6144 transitions
   ‚Üì
6. GAE (training/trainer.py)
   ‚Üì
   Advantages calcul√©s
   ‚Üì
7. PPO (agents/ppo.py)
   ‚Üì
   Optimisation de la politique
   ‚Üì
8. REPEAT (avec curriculum + eval p√©riodique)
```

---

## üìö R√©f√©rences th√©oriques

### Papers fondamentaux:
1. **PPO**: "Proximal Policy Optimization Algorithms" (Schulman et al., 2017)
2. **GAE**: "High-Dimensional Continuous Control Using Generalized Advantage Estimation" (Schulman et al., 2016)
3. **LSTM**: "Long Short-Term Memory" (Hochreiter & Schmidhuber, 1997)
4. **Curriculum Learning**: "Curriculum Learning" (Bengio et al., 2009)

### Concepts cl√©s:
- **On-policy**: L'agent apprend de sa propre politique actuelle
- **Actor-Critic**: Deux r√©seaux (policy + value) qui s'entraident
- **Advantage**: Mesure de la qualit√© d'une action par rapport √† la moyenne
- **Clipping**: Limite les changements de politique pour la stabilit√©
- **Entropy**: Mesure de l'al√©atoire (exploration vs exploitation)

---

## üí° Conseils pour comprendre le code

1. **Commencez par les concepts:**
   - Lisez d'abord cette doc
   - Regardez des vid√©os sur PPO (ex: Arxiv Insights)
   - Comprenez le probl√®me avant la solution

2. **Suivez le flow:**
   - Partez de `scripts/train.py`
   - Suivez l'ex√©cution step by step
   - Utilisez un debugger pour voir les tensors

3. **Exp√©rimentez:**
   - Changez les hyperparam√®tres
   - Observez l'impact sur l'apprentissage
   - Utilisez TensorBoard pour visualiser

4. **Lisez les papers:**
   - PPO paper (tr√®s accessible)
   - GAE paper (pour les advantages)
   - Google Research Football paper (pour l'env)

---

**Auteur:** Documentation g√©n√©r√©e pour le projet Football RL
**Date:** 2025
**Version:** 1.0
