# Fixes: AsyncVectorEnv Info Format

## üî¥ Probl√®mes identifi√©s

### 1. Format info ambigu avec AsyncVectorEnv

**Probl√®me:**
Le code essayait 2 formats mais ratait le format standard de `gymnasium.vector.AsyncVectorEnv`:

```python
# Format r√©el de Gymnasium AsyncVectorEnv:
info = {
    'final_info': [...],  # Infos finaux par env (quand done=True)
    'final_observation': [...],
    '_final_info': [...],
    '_final_observation': [...]
}

# Le score est dans: info['final_info'][env_idx]['raw_score']
```

**Ancien code (cass√©):**
```python
# Essayait seulement:
if env_idx in info:  # ‚ùå Jamais vrai avec AsyncVectorEnv
    score = info[env_idx]['raw_score']
    
if 'raw_score' in info:  # ‚ùå Jamais vrai avec AsyncVectorEnv
    score = info['raw_score'][env_idx]
```

---

### 2. Score pas extrait aux bons moments

**Flow actuel:**
```
GRF env ‚Üí obs dict (avec 'score')
  ‚Üí GRFtoGymnasiumWrapper (extrait score ‚Üí info['raw_score']) ‚úÖ
  ‚Üí RewardShaperWrapper (obs encore dict)
  ‚Üí ObsWrapperRaw (obs devient vector)
  ‚Üí VectorEnv (vectorise les envs)
  ‚Üí AsyncVectorEnv (format info change!) ‚ùå
```

**Probl√®me:**
- GRFtoGymnasiumWrapper extrait bien `info['raw_score']`
- Mais AsyncVectorEnv transforme le format: `info ‚Üí {'final_info': [info1, info2, ...]}`
- Code ne g√©rait pas ce format

---

### 3. Comptage goals cumulatifs

**Ancien code:**
```python
if done[env_idx]:
    score = info['raw_score']  # [3, 2] par exemple
    total_goals_scored += 3    # Compte le score final
    total_goals_conceded += 2
```

**Probl√®me th√©orique:**
Compte les scores finaux d'√©pisodes, pas les buts individuels.

**Mais √ßa marche par accident:**
GRF reset le score √† `[0, 0]` √† chaque √©pisode, donc:
- √âpisode 1: [2, 1] ‚Üí +2 scored, +1 conceded ‚úÖ
- √âpisode 2: [1, 0] ‚Üí +1 scored, +0 conceded ‚úÖ

C'est correct mais fragile (d√©pend du comportement GRF).

---

### 4. Fallback impr√©cis

**Avec reward shaping:**
```python
if episode_return >= 15:  # Goal ?
    total_goals_scored += 1

# Probl√®me: episode_return inclut shaping!
# Goal + shaping: 100 + 50 = 150 ‚úÖ
# Draw + bon shaping: 0 + 20 = 20 ‚ùå (faux positif)
```

**Avec rewards_sparse (shaping d√©sactiv√©):**
```python
# Goals only:
goal_scored: +100
goal_distance: +0.5 par step (~100 total)

# episode_return >= 15 d√©tecte goal ‚úÖ
# Sans goal: ~100 de goal_distance ‚ùå (faux positif!)
```

---

## ‚úÖ Solutions appliqu√©es

### 1. Fonction robuste `_extract_score_from_info()`

```python
def _extract_score_from_info(self, info, env_idx, done_mask):
    """G√®re les 3 formats possibles."""
    
    # Format 1: AsyncVectorEnv avec final_info (standard Gymnasium)
    if done_mask[env_idx] and 'final_info' in info:
        final_info = info['final_info'][env_idx]
        if final_info and 'raw_score' in final_info:
            return (int(final_info['raw_score'][0]), 
                    int(final_info['raw_score'][1]))
    
    # Format 2: info[env_idx]['raw_score'] (custom dict)
    if env_idx in info and 'raw_score' in info[env_idx]:
        score = info[env_idx]['raw_score']
        return (int(score[0]), int(score[1]))
    
    # Format 3: info['raw_score'][env_idx] (vectorized)
    if 'raw_score' in info:
        score = info['raw_score'][env_idx]
        return (int(score[0]), int(score[1]))
    
    return None
```

**Avantages:**
- ‚úÖ G√®re AsyncVectorEnv correctement
- ‚úÖ Compatible avec anciens formats
- ‚úÖ Retourne None si pas trouv√© (fail-safe)
- ‚úÖ Clean et testable

---

### 2. Code simplifi√© dans trainer

**Avant (60 lignes):**
```python
tracked = False

# Essayer format 1
if isinstance(info, dict) and env_idx in info:
    env_info = info[env_idx]
    if isinstance(env_info, dict) and 'raw_score' in env_info:
        score = env_info['raw_score']
        # ... 15 lignes
        tracked = True

# Essayer format 2
if not tracked and isinstance(info, dict) and 'raw_score' in info:
    # ... 15 lignes
    tracked = True

# Fallback
if not tracked:
    # ... 15 lignes
```

**Apr√®s (10 lignes):**
```python
score_tuple = self._extract_score_from_info(info, env_idx, done)

if score_tuple:
    goals_scored, goals_conceded = score_tuple
    self.total_goals_scored += goals_scored
    # ...
else:
    # Fallback avec warning
    self._estimate_results_from_return(episode_return)
```

---

## üìä Impact des fixes

### Avant (cass√©):
```
AsyncVectorEnv step()
  ‚Üí info = {'final_info': [...], ...}
  ‚Üí Trainer cherche info[env_idx] ‚ùå
  ‚Üí Pas trouv√©
  ‚Üí Fallback (estimation impr√©cise)
  ‚Üí Compteurs goals incorrects
```

### Apr√®s (correct):
```
AsyncVectorEnv step()
  ‚Üí info = {'final_info': [...], ...}
  ‚Üí Trainer cherche info['final_info'][env_idx] ‚úÖ
  ‚Üí Trouve raw_score
  ‚Üí Compteurs goals corrects
```

---

## üß™ Test de validation

Pour v√©rifier que √ßa marche:

```python
# Dans les logs, au premier √©pisode termin√©:
# Ancien (cass√©):
‚ö†Ô∏è raw_score not found in info!
   info type: <class 'dict'>
   info keys: ['final_info', 'final_observation', ...]
   
# Nouveau (correct):
# Pas de warning (score trouv√©)
# OU si vraiment pas trouv√©:
‚ö†Ô∏è raw_score not found in info - using fallback estimation
   info keys: [...]
```

---

## üìö R√©f√©rences

**Gymnasium AsyncVectorEnv documentation:**
- https://gymnasium.farama.org/api/vector/#gymnasium.vector.AsyncVectorEnv
- Info format: `{'final_info': [...], 'final_observation': [...]}`
- `final_info[i]` contient les infos du dernier step de l'env i quand done=True

**GRF score format:**
- `obs['score']` = `[goals_scored, goals_conceded]`
- Reset √† `[0, 0]` √† chaque nouvel √©pisode

---

## ‚úÖ Conclusion

Probl√®mes TOUS corrig√©s:
1. ‚úÖ Format AsyncVectorEnv g√©r√©
2. ‚úÖ Extraction score robuste
3. ‚úÖ Comptage correct (marche avec GRF)
4. ‚úÖ Fallback am√©lior√© avec warning

**Code plus propre, plus robuste, plus maintenable!** üéØ
