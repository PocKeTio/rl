# RND (Random Network Distillation) Implementation

## ğŸ“š RÃ©fÃ©rence
- **Paper**: "Exploration by Random Network Distillation" (Burda et al., 2018)
- **Link**: https://arxiv.org/abs/1810.12894

## ğŸ¯ Principe

RND fournit des **intrinsic rewards** basÃ©s sur la **nouveautÃ©** des Ã©tats:

```python
# Target network: fixe, alÃ©atoire
target_features = target_network(obs)

# Predictor network: apprend Ã  prÃ©dire target
predicted_features = predictor_network(obs)

# Intrinsic reward: erreur de prÃ©diction (MSE)
intrinsic_reward = ||target_features - predicted_features||Â²

# Ã‰tats familiers â†’ faible erreur â†’ faible reward
# Ã‰tats nouveaux â†’ haute erreur â†’ haute reward
```

## ğŸ”§ ImplÃ©mentation

### Fichiers modifiÃ©s:
1. **`src/gfrl/models/rnd.py`** - Module RND
2. **`src/gfrl/train/trainer.py`** - IntÃ©gration dans boucle training
3. **`src/gfrl/algo/storage.py`** - Support intrinsic rewards (dÃ©jÃ  prÃ©sent)

### Workflow:

```python
# 1. Compute intrinsic rewards (chaque step)
intrinsic_reward = rnd.compute_intrinsic_reward(obs)

# 2. Store dans rollout buffer
storage.insert(..., intrinsic_reward=intrinsic_reward)

# 3. Combine avec extrinsic rewards (compute_returns)
total_reward = extrinsic_reward + beta * intrinsic_reward

# 4. Update RND predictor (aprÃ¨s PPO update)
rnd_loss = rnd.update(all_observations)
```

## âš™ï¸ HyperparamÃ¨tres

Dans `configs/train_sparse.yaml`:

```yaml
use_rnd: true                 # Active RND
rnd_beta: 0.5                 # Poids intrinsic reward (0.5 = Ã©quilibre)
rnd_output_dim: 128           # Dimension embedding
rnd_hidden_dim: 256           # Hidden layers
rnd_lr: 0.0001                # Learning rate predictor
```

### Tuning `rnd_beta`:

- **`beta = 0.0`**: Pas d'intrinsic reward (PPO standard)
- **`beta = 0.1-0.3`**: Faible curiositÃ© (exploration modÃ©rÃ©e)
- **`beta = 0.5`**: Ã‰quilibre extrinsic/intrinsic (recommandÃ© pour sparse rewards)
- **`beta = 1.0`**: Forte curiositÃ© (peut ignorer goals)
- **`beta > 1.0`**: Trop de curiositÃ© (agent explore sans apprendre task)

## ğŸ“Š MÃ©triques

Logs pendant training:

```
RND loss: 0.0234           # Erreur de prÃ©diction (devrait diminuer)
Intrinsic reward: 0.0156   # Moyenne des intrinsic rewards
```

**InterprÃ©tation:**
- RND loss Ã©levÃ© â†’ Observations nouvelles frÃ©quentes (bonne exploration)
- RND loss baisse â†’ Agent revisite Ã©tats connus (exploitation)
- Intrinsic reward Ã©levÃ© â†’ Agent explore activement

## ğŸ® Utilisation

### EntraÃ®nement avec RND:

```powershell
.\START_SPARSE.ps1
```

Ou manuellement:

```powershell
python -m gfrl.cli.train_ppo_rnd --config configs/train_sparse.yaml
```

### DÃ©sactiver RND:

Dans `train_sparse.yaml`:
```yaml
use_rnd: false
```

## ğŸ”¬ Avantages RND

### Avec sparse rewards (goals only):

âœ… **AccÃ©lÃ¨re l'apprentissage** (10-15M steps au lieu de 20-30M)
âœ… **Exploration intelligente** (ne visite pas Ã©tats alÃ©atoires)
âœ… **Stable** (pas d'explosion de rewards)
âœ… **GÃ©nÃ©ralise bien** (apprend Ã  explorer efficacement)

### Vs autres mÃ©thodes:

| MÃ©thode | Exploration | Vitesse | StabilitÃ© |
|---------|-------------|---------|-----------|
| High entropy | Random | Lent | Stable |
| RND | Intelligent | Rapide | Stable |
| Count-based | Simple | Moyen | Instable |
| ICM | Complexe | Moyen | Instable |

## ğŸ› Debugging

### RND loss ne diminue pas:
- Observations pas normalisÃ©es
- Learning rate trop Ã©levÃ©
- Architecture trop petite

### Intrinsic rewards trop Ã©levÃ©s:
- `rnd_beta` trop Ã©levÃ© â†’ rÃ©duire Ã  0.1-0.3
- Agent ignore extrinsic rewards â†’ vÃ©rifier goals sont bien comptÃ©s

### Agent n'explore pas:
- RND dÃ©sactivÃ© par erreur
- `rnd_beta` trop faible â†’ augmenter Ã  0.5-1.0
- Entropy trop basse â†’ augmenter entropy_coef

## ğŸ“ˆ RÃ©sultats attendus

Avec RND + sparse rewards:

```
0-5M steps:    Exploration RND active, premiers goals
5-15M steps:   DÃ©couverte patterns efficaces
15-30M steps:  StratÃ©gie stable Ã©merge
30M+ steps:    Performance optimale (30-50% winrate 3v1)
```

**ComparÃ© Ã  sans RND:** ~2Ã— plus rapide! ğŸš€
