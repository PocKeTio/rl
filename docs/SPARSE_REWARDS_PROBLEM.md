# ProblÃ¨me: Rewards TROP Sparse avec RND

## ğŸ”´ ProblÃ¨me observÃ© (2.3M steps)

### Comportement:
```
Agent court vers SES PROPRES buts avec la balle
Ne tire JAMAIS
0 goals sur 2.3M steps
Mean reward: 0.0
Entropy: 2.73 (93% random!)
```

### MÃ©triques:
```
Goals scored: 0.00/game
Winrate: 0.2%
Drawrate: 94.3%

RND intrinsic: 1.6030
Policy loss: 0.0011 (pas d'apprentissage)
Entropy: 2.7329 (quasi-uniforme)
```

---

## ğŸ”¬ Analyse Root Cause

### Configuration initiale:
```yaml
# rewards_sparse.yaml
goal_scored: +100
goal_conceded: -10
# TOUT LE RESTE DÃ‰SACTIVÃ‰

# train_sparse.yaml
rnd_beta: 0.05
```

### Calcul du signal:

**Sur 223 steps (durÃ©e moyenne Ã©pisode):**

```python
# Extrinsic (task rewards)
Goals: 0 Ã— 100 = 0
Total extrinsic: 0

# Intrinsic (RND curiosity)
RND par step: 1.6
Beta: 0.05
Contribution par step: 0.05 Ã— 1.6 = 0.08
Total intrinsic: 0.08 Ã— 223 = 17.84

# Ratio
Extrinsic/Intrinsic = 0 / 17.84 = 0:âˆ
â†’ Agent optimise UNIQUEMENT RND!
```

---

## ğŸ’¡ Pourquoi l'agent va vers ses buts?

### Logique RND:
```
RND reward = MSE entre target et predictor
Ã‰tats jamais visitÃ©s = haute MSE = haute reward

Zone rarement visitÃ©e: Ses propres buts
â†’ RND reward Ã©levÃ©
â†’ Agent y va pour maximiser curiositÃ©
â†’ Ignore complÃ¨tement la task (scorer)
```

### Cercle vicieux:
```
1. Agent n'a jamais marquÃ© â†’ pas de signal task
2. Optimise RND uniquement
3. RND le pousse vers zones inexplorÃ©es (mauvaises)
4. Ne marque jamais â†’ retour Ã  1
```

---

## âŒ Pourquoi Sparse + RND Ã©choue

### HypothÃ¨se initiale (fausse):
```
"RND va guider l'agent Ã  explorer intelligemment
 et finir par trouver comment marquer"
```

### RÃ©alitÃ©:
```
RND pousse vers NOUVEAUTÃ‰, pas vers TASK

Zones nouvelles â‰  Zones utiles pour task
â†’ Agent explore alÃ©atoirement
â†’ Ne converge JAMAIS vers la task
```

### Comparaison:

| Approche | Signal task | Exploration | RÃ©sultat |
|----------|-------------|-------------|----------|
| **Dense rewards** | Fort, constant | GuidÃ©e par shaping | Apprend vite mais bugs |
| **Sparse + RND (beta=0.05)** | ZÃ©ro (pas de goals) | DominÃ©e par RND | Explore sans apprendre |
| **Sparse + minimal shaping** | Faible mais constant | Ã‰quilibrÃ©e | Apprend lentement mais stable |

---

## âœ… Solution appliquÃ©e

### 1. Ajouter guidance minimale

```yaml
# rewards_sparse.yaml
goal_distance:
  enabled: true
  reward_scale: 0.5  # TRÃˆS FAIBLE
  normalize: true
```

**Impact:**
```python
Goal distance reward: ~0.1-1.0 par step (vers but adverse)
Sur 223 steps: ~50-100 de signal task
Avec goals: +100 supplÃ©mentaires

â†’ Signal task > Signal RND
```

### 2. RÃ©duire rnd_beta

```yaml
# train_sparse.yaml
rnd_beta: 0.01  # Was 0.05
```

**Impact:**
```python
RND contribution: 0.01 Ã— 1.6 Ã— 223 = 3.57
Goal distance: ~75
Goals (si marquÃ©s): +100

Ratio task/curiosity: 175 / 3.57 = 49:1 âœ…
â†’ RND est vraiment un bonus, pas l'objectif
```

---

## ğŸ“Š Nouveau calcul des rewards

### Avec minimal shaping (0.5 goal_distance):

```python
# Par Ã©pisode (223 steps):

Extrinsic:
- Goal distance: ~0.3 moyenne/step Ã— 223 = 67
- Goal scored: 100 (si marquÃ©)
- Total: 167 (si goal) ou 67 (si pas goal)

Intrinsic:
- RND: 0.01 Ã— 1.6 Ã— 223 = 3.57

Ratio: 167 / 3.57 = 47:1 (avec goal)
       67 / 3.57 = 19:1 (sans goal)

â†’ Task dominant, RND est un bonus âœ…
```

---

## ğŸ¯ Lessons Learned

### âŒ Sparse "pur" ne fonctionne PAS avec RND
```
MÃªme avec RND, besoin d'UN MINIMUM de guidance
Sinon: Agent optimise curiositÃ© au lieu de task
```

### âœ… Sparse "quasi-pur" fonctionne
```
Goal scored: +100 (dominant)
Goal distance: +0.5 (minimal, juste orientation)
RND: 0.01 beta (vraiment un bonus)

â†’ Balance: Task >> Shaping >> Curiosity
```

### ğŸ“ RÃ¨gle gÃ©nÃ©rale:
```
Signal extrinsic minimum > Signal intrinsic total

Sinon: RND domine et agent n'apprend pas la task
```

---

## ğŸš€ Attentes avec nouvelle config

### Timeline prÃ©vue:

```
0-5M steps:
  Goal distance guide vers but adverse
  RND encourage essayer diffÃ©rentes actions
  Premiers goals par essai-erreur
  Winrate: 1-5%

5-15M steps:
  Agent comprend: aller vers but = rÃ©compense
  Commence Ã  tirer plus rÃ©guliÃ¨rement
  Winrate: 5-15%

15-30M steps:
  StratÃ©gie Ã©merge
  Tir + placement
  Winrate: 20-35%
```

---

## ğŸ“š RÃ©fÃ©rences thÃ©oriques

**Burda et al. (2018) - RND paper:**
- "Intrinsic rewards should be smaller than extrinsic rewards"
- Beta range: 0.01-0.1
- Utilisent TOUJOURS des task rewards non-nuls

**OpenAI Procgen (2020):**
- "Pure exploration (RND only) fails on most tasks"
- "Need both task signal and exploration bonus"

**Best practice:**
```
beta_intrinsic = 0.01-0.05
Minimal task shaping (mÃªme sparse)
Never pure exploration without any task signal
```
