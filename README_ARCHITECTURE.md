# ğŸ† Google Research Football - Architecture du Projet

Ce document dÃ©taille les fichiers utilisÃ©s dans le pipeline d'entraÃ®nement et leur rÃ´le.

---

## ğŸ“‹ Table des matiÃ¨res

- [Configs](#configs)
- [Code source](#code-source)
- [Scripts](#scripts)
- [Pipeline d'exÃ©cution](#pipeline-dexÃ©cution)

---

## ğŸ“ Configs

Fichiers YAML de configuration situÃ©s dans `configs/`.

### `train_5v5.yaml`
**RÃ´le**: Configuration principale de l'entraÃ®nement PPO

**Contenu**:
- HyperparamÃ¨tres PPO (learning rate, clip_epsilon, gamma, GAE lambda)
- Entropy annealing (0.01 â†’ 0.001 sur 5M steps)
- Batch size et architecture (num_envs, rollout_len, ppo_epochs)
- RND (exploration intrinsÃ¨que, actuellement dÃ©sactivÃ©)
- Logging et checkpointing

**UtilisÃ© par**: `src/gfrl/train/trainer.py`

---

### `env.yaml`
**RÃ´le**: Configuration de l'environnement GRF

**Contenu**:
- Nom du scÃ©nario (`academy_empty_goal_close`)
- Type de reprÃ©sentation (`raw` - observations brutes)
- Action set (`sticky` - 19 actions)
- Nombre d'environnements parallÃ¨les (`num_envs: 24`)
- Frame skip et paramÃ¨tres de rendering

**UtilisÃ© par**: `src/gfrl/env/make_env.py`

---

### `rewards_dense.yaml`
**RÃ´le**: Configuration du reward shaping dense

**Contenu**:
- **Sparse rewards**: goal_scored (+5.0), own_goal (-10.0), win_bonus (+2.0)
- **Dense rewards**: 
  - `goal_distance` (+0.08/step) - Guide vers le but
  - `penalty_box_bonus` (+0.15/step) - Encourage entrÃ©e surface
  - `shot_attempt` (+1.5 si xâ‰¥0.75) - RÃ©compense tirs proches
  - `ball_distance` (+0.02/step) - Guide vers ballon
  - `dribble_progress` (+0.05/step) - Progression avec ballon
- **PÃ©nalitÃ©s anti-CSC**: dangerous_shot, approach_own_goal
- **PÃ©nalitÃ©s hors limites**: out_of_bounds (-1.0)
- **Shaping annealing**: RÃ©duit progressivement les dense rewards (1.0 â†’ 0.1 sur 10M steps)

**UtilisÃ© par**: `src/gfrl/env/rewarders.py`

---

### `curriculum_5v5.yaml`
**RÃ´le**: DÃ©finit les phases de curriculum learning

**Contenu**:
- SÃ©quence de scÃ©narios (3v1, 3v2, 3v3, 5v5)
- CritÃ¨res de succÃ¨s (winrate_min, min_episodes)
- DurÃ©e de chaque phase (duration_steps)
- Auto-advance settings

**UtilisÃ© par**: `src/gfrl/train/trainer.py`

**Note**: Phase 0 (empty_goal) actuellement commentÃ©e, dÃ©marre directement en 3v1.

---

## ğŸ’» Code Source

Fichiers Python situÃ©s dans `src/gfrl/`.

### Training Pipeline

#### `train/trainer.py`
**RÃ´le**: Orchestrateur principal de l'entraÃ®nement

**ResponsabilitÃ©s**:
- Initialise l'environnement et le modÃ¨le
- GÃ¨re le curriculum learning (transitions entre phases)
- Boucle d'entraÃ®nement principale:
  1. Collecte rollouts (256 steps Ã— 24 envs)
  2. Calcule GAE returns
  3. Lance PPO update (4 epochs)
  4. Update RND (si activÃ©)
  5. Logging et checkpointing
- GÃ¨re les LSTM states entre steps
- Reset LSTM states sur `done`

**DÃ©pendances**: PPO, RolloutStorage, Policy, Environnements

---

#### `algo/ppo.py`
**RÃ´le**: ImplÃ©mentation de l'algorithme PPO

**ResponsabilitÃ©s**:
- Calcul des losses:
  - **Policy loss**: Clipped surrogate objective
  - **Value loss**: Clipped MSE avec returns
  - **Entropy loss**: Bonus pour exploration
- Gradient clipping (max_grad_norm=0.5)
- AMP (Automatic Mixed Precision) support
- KL divergence early stopping
- Gestion des sÃ©quences LSTM (full batch mode)

**Fonction clÃ©**: `update(storage)` - EntraÃ®ne la policy sur les rollouts collectÃ©s

---

#### `algo/storage.py`
**RÃ´le**: Buffer de rollouts pour PPO

**ResponsabilitÃ©s**:
- Stocke les transitions (obs, actions, rewards, values, lstm_states)
- Calcul GAE (Generalized Advantage Estimation):
  - Returns = advantages + values
  - Masking sur `done` pour reset
- GÃ©nÃ©rateur de batches:
  - **Full batch mode** (mini_batch_size=None): SÃ©quences (num_envs, num_steps)
  - **Mini-batch mode**: Random shuffling
- Normalisation des advantages

**Note**: Full batch obligatoire pour LSTM (prÃ©serve temporalitÃ©)

---

### ModÃ¨les

#### `models/policy_lstm.py`
**RÃ´le**: Architecture du rÃ©seau policy-value LSTM

**Architecture**:
```
Input (obs_dim=115)
  â†“
Backbone MLP (256 â†’ 256 â†’ 256) + LayerNorm
  â†“
LSTM (2 layers, 128 hidden)
  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
Policy Head   Value Head
(19 actions)  (1 value)
```

**SpÃ©cificitÃ©s**:
- **Policy head init**: `orthogonal_(gain=0.1)` - Crucial pour apprentissage
- **LSTM state management**: Reset sur Ã©pisodes terminÃ©s
- **Sequence handling**: Support batch (B, obs_dim) et sÃ©quence (B, T, obs_dim)

**Fonctions**:
- `forward()`: Policy logits + value
- `get_action()`: Sample action depuis policy
- `evaluate_actions()`: Log probs pour PPO update

---

### Environnement

#### `env/make_env.py`
**RÃ´le**: Factory pour crÃ©er les environnements GRF

**ResponsabilitÃ©s**:
- CrÃ©e l'env GRF de base (`football_env.create_environment`)
- Applique les wrappers dans l'ordre:
  1. `GRFtoGymnasiumWrapper` - Conversion Gym â†’ Gymnasium
  2. `MirrorWrapper` - Data augmentation (optionnel)
  3. `RewardShaperWrapper` - Applique reward shaping
  4. `ObsWrapperRaw` - Encode observations raw en vecteur dense
  5. `RunningNormWrapper` - Normalise observations
- Support vectorisation (AsyncVectorEnv)

**Fonction clÃ©**: `create_vec_envs()` - CrÃ©e 24 envs parallÃ¨les

---

#### `env/wrappers.py`
**RÃ´le**: Wrappers Gymnasium pour GRF

**Classes principales**:

1. **`GRFtoGymnasiumWrapper`**: Conversion Gym (old) â†’ Gymnasium (new)
   - Adapte API step/reset
   
2. **`RewardShaperWrapper`**: Applique reward shaping
   - Multiplie goal rewards (Ã—5 pour goals, Ã—2 pour wins)
   - DÃ©tecte CSC (own goals) et applique pÃ©nalitÃ© massive (-10.0)
   - Ajoute shaped rewards via `RewardShaper.compute_shaping()`
   - Ajoute win/draw/loss bonus en fin d'Ã©pisode

3. **`ObsWrapperRaw`**: Encode observations
   - Utilise `RawObsEncoder` pour vectoriser
   - Observations â†’ vecteur dense (115 dims)

4. **`RunningNormWrapper`**: Normalisation running
   - Normalise observations avec mean/std cumulatifs

---

#### `env/rewarders.py`
**RÃ´le**: Calcul du reward shaping modulaire

**Classe**: `RewardShaper`

**MÃ©thode principale**: `compute_shaping(prev_obs, obs, action, info)`

**Calcule**:
- **Shot rewards** (avec zone check xâ‰¥0.75)
- **Pass rewards** (bonus si sous pression)
- **Ball possession/recovery**
- **Dense shaping** (annealÃ© progressivement):
  - `goal_distance`: Se rapprocher du but (clippÃ© Ã  xâ‰¤1.0)
  - `penalty_box_bonus`: ÃŠtre dans la surface
  - `dribble_progress`: Progresser avec ballon (clippÃ© Ã  xâ‰¤1.0)
- **PÃ©nalitÃ©s anti-CSC** (jamais annealÃ©es):
  - Tir vers propre but
  - Passe dangereuse en dÃ©fense
  - Approche propre but avec ballon
- **Out of bounds penalty**: Forte pÃ©nalitÃ© (-1.0) si x>1.0 ou hors terrain

**Annealing**: Multiplie shaped rewards par `annealing_multiplier` (1.0 â†’ 0.1 sur 10M steps)

---

#### `env/encoders.py`
**RÃ´le**: Encode observations GRF raw en vecteurs

**Classe**: `RawObsEncoder`

**Encode** (115 dimensions):
- **Ball** (3): [x, y, z]
- **Ball direction** (3): [dx, dy, dz]
- **Ball owned** (3): [team, player_idx, one-hot]
- **Active player** (88): Position, direction, vitesse, etc. (8 dims)
- **Teammates** (10 Ã— 8 = 80): Positions relatives, directions
- **Adversaires** (11 Ã— 8 = 88): Idem
- **Game mode** (7): one-hot encoding

**Note**: Normalise positions pour Ãªtre dans [-1, 1]

---

## ğŸš€ Scripts

Scripts d'exÃ©cution situÃ©s dans `src/gfrl/cli/`.

### `train_ppo_rnd.py`
**RÃ´le**: Point d'entrÃ©e pour l'entraÃ®nement

**Usage**:
```bash
python -m gfrl.cli.train_ppo_rnd --config configs/train_5v5.yaml
```

**ResponsabilitÃ©s**:
- Parse arguments CLI
- Charge configs (train, env, rewards, curriculum)
- Initialise `Trainer`
- Lance `trainer.train()`

---

### `scripts/watch_agent.py`
**RÃ´le**: Visualise un agent entraÃ®nÃ©

**Usage**:
```bash
python scripts/watch_agent.py --checkpoint checkpoints/last.pt --render --env academy_3_vs_1_with_keeper --num-games 50
```

**ResponsabilitÃ©s**:
- Charge checkpoint
- CrÃ©e env avec rendering
- Roule agent en mode dÃ©terministe
- Affiche stats (goals, winrate)

---

## ğŸ”„ Pipeline d'ExÃ©cution

### 1. DÃ©marrage
```
train_ppo_rnd.py
  â”œâ”€ Load configs (train_5v5.yaml, env.yaml, rewards_dense.yaml, curriculum_5v5.yaml)
  â”œâ”€ Create Trainer
  â””â”€ trainer.train()
```

### 2. Initialisation Trainer
```
Trainer.__init__()
  â”œâ”€ Create RewardShaper (rewards_dense.yaml)
  â”œâ”€ Create vectorized envs (24 parallel)
  â”‚   â””â”€ Apply wrappers (RewardShaper, ObsEncoder, RunningNorm)
  â”œâ”€ Create PolicyLSTM (obs_dim=115, action_dim=19)
  â”œâ”€ Create PPO optimizer
  â”œâ”€ Create RolloutStorage (num_steps=256, num_envs=24)
  â””â”€ Load curriculum phases
```

### 3. Boucle d'EntraÃ®nement
```
For each update (8138 updates = 50M steps):
  
  # 1. Collect rollouts (256 steps Ã— 24 envs = 6144 samples)
  For step in range(256):
    â”œâ”€ obs â†’ policy.get_action() â†’ action, log_prob, value, lstm_state
    â”œâ”€ envs.step(action) â†’ next_obs, reward, done, info
    â”œâ”€ storage.insert(obs, action, log_prob, value, reward, done, lstm_state)
    â””â”€ If done: Reset LSTM state for that env
  
  # 2. Compute GAE returns
  â”œâ”€ next_value = policy.get_value(last_obs)
  â””â”€ storage.compute_returns(next_value, gamma=0.993, gae_lambda=0.95)
  
  # 3. PPO update (4 epochs, full batch pour LSTM)
  â”œâ”€ For epoch in range(4):
  â”‚   â””â”€ batch = storage.get_generator(batch_size=6144, mini_batch_size=None)
  â”‚       â”œâ”€ Reshape to sequences: (num_envs=24, num_steps=256, obs_dim)
  â”‚       â”œâ”€ policy.evaluate_actions(obs_seq, actions_seq, lstm_state)
  â”‚       â”œâ”€ Compute losses (policy + value + entropy)
  â”‚       â”œâ”€ Backward + gradient clip
  â”‚       â””â”€ Optimizer step
  
  # 4. Update RND (si enabled)
  â””â”€ rnd.update() - CuriositÃ© intrinsÃ¨que
  
  # 5. Logging & Checkpointing
  â”œâ”€ Log metrics (rewards, entropy, losses, KL)
  â”œâ”€ Check curriculum advancement
  â””â”€ Save checkpoint every 50 updates
```

### 4. Reward Computation (par step)
```
env.step(action)
  â”œâ”€ base_reward = GRF sparse reward (Â±1 pour goals)
  â”‚
  â”œâ”€ RewardShaperWrapper:
  â”‚   â”œâ”€ Multiply goal rewards: base_reward Ã— goal_scored (Ã—5)
  â”‚   â”œâ”€ Detect CSC: Si goal concÃ©dÃ© en academy no-opponent â†’ -10.0
  â”‚   â”‚
  â”‚   â”œâ”€ RewardShaper.compute_shaping():
  â”‚   â”‚   â”œâ”€ Shot rewards (+1.5 si xâ‰¥0.75, pÃ©nalitÃ© sinon)
  â”‚   â”‚   â”œâ”€ Pass rewards (+0.2, +0.1 si sous pression)
  â”‚   â”‚   â”œâ”€ Dense shaping (annealÃ©):
  â”‚   â”‚   â”‚   â”œâ”€ goal_distance: +0.08 Ã— progress (clippÃ© xâ‰¤1.0)
  â”‚   â”‚   â”‚   â”œâ”€ penalty_box_bonus: +0.15 si xâ‰¥0.75
  â”‚   â”‚   â”‚   â””â”€ dribble_progress: +0.05 Ã— progress (clippÃ© xâ‰¤1.0)
  â”‚   â”‚   â”œâ”€ Anti-CSC penalties (non-annealÃ©)
  â”‚   â”‚   â””â”€ Out of bounds: -1.0 si x>1.0 ou hors terrain
  â”‚   â”‚
  â”‚   â”œâ”€ shaped_reward *= annealing_multiplier
  â”‚   â”œâ”€ total_reward = base_reward + shaped_reward
  â”‚   â”‚
  â”‚   â””â”€ If episode done: total_reward += win/draw/loss bonus
  â”‚
  â””â”€ Return (obs, total_reward, done, info)
```

---

## ğŸ› Bugs CorrigÃ©s

### Bug #1: Policy head gain trop faible
- **Avant**: `gain=0.01` â†’ logits quasi-nuls â†’ distribution uniforme
- **AprÃ¨s**: `gain=0.1` â†’ apprentissage possible
- **Fichier**: `src/gfrl/models/policy_lstm.py`

### Bug #2: Mini-batches cassent LSTM
- **Avant**: Mini-batches avec LSTM state=0 â†’ casse temporalitÃ©
- **AprÃ¨s**: Full batch mode (mini_batch_size=None)
- **Fichier**: `src/gfrl/train/trainer.py`

### Bug #3: Shape mismatch LSTM
- **Avant**: Flatten (num_steps Ã— num_envs) + LSTM state (num_envs) â†’ crash
- **AprÃ¨s**: Reshape en sÃ©quences (num_envs, num_steps, obs_dim)
- **Fichier**: `src/gfrl/algo/storage.py`

### Bug #4: Goal rewards non appliquÃ©s
- **Avant**: goal_scored config ignorÃ©, goals valent +1
- **AprÃ¨s**: MultipliÃ© par config (Ã—5)
- **Fichier**: `src/gfrl/env/wrappers.py`

### Bug #5: Agent court derriÃ¨re le but
- **Avant**: `abs(x - 1.0)` â†’ x=1.1 a mÃªme distance que x=0.9
- **AprÃ¨s**: Clip Ã  xâ‰¤1.0 + pÃ©nalitÃ© out_of_bounds
- **Fichier**: `src/gfrl/env/rewarders.py`

---

## ğŸ“Š Configuration Actuelle

**HyperparamÃ¨tres PPO**:
- Learning rate: 3e-4
- Gamma: 0.993
- GAE lambda: 0.95
- Clip epsilon: 0.2
- Entropy: 0.01 â†’ 0.001 (5M steps)
- Batch: 6144 (full batch pour LSTM)
- Epochs: 4

**Architecture**:
- Backbone: MLP 256Ã—3 + LayerNorm
- LSTM: 2 layers, 128 hidden
- Policy head gain: 0.1 âœ…

**Rewards** (config Ã©quilibrÃ©e post-bugfix):
- goal_scored: +5.0
- win_bonus: +2.0
- shot_attempt: +1.5 (si xâ‰¥0.75)
- penalty_box_bonus: +0.15/step (si xâ‰¥0.75)
- goal_distance: +0.08/step
- own_goal: -10.0
- out_of_bounds: -1.0

**Environnement**:
- 24 envs parallÃ¨les
- Rollout: 256 steps
- ReprÃ©sentation: raw (115 dims)
- Action set: sticky (19 actions)

---

## ğŸ¯ Fichiers NON UtilisÃ©s

Ces fichiers existent mais ne sont **pas** dans le pipeline actuel:

- `src/gfrl/algo/rnd.py` - RND dÃ©sactivÃ© (use_rnd: false)
- `src/gfrl/models/cnn_policy.py` - On utilise raw obs, pas pixels
- `scripts/eval_agent.py` - Ã‰valuation formelle non utilisÃ©e
- Tout autre fichier non mentionnÃ© ci-dessus

---

**Date**: 20 octobre 2025  
**Version**: Post-bugfix (5 bugs critiques corrigÃ©s)  
**Status**: PrÃªt pour entraÃ®nement sur 3v1 â†’ 3v2 â†’ 3v3 â†’ 5v5
