# Google Research Football - PPO + RND

ImplÃ©mentation PPO (Proximal Policy Optimization) avec RND (Random Network Distillation) pour l'environnement Google Research Football.

## ğŸ¯ Features

- **PPO Algorithm** avec LSTM pour mÃ©moire temporelle
- **RND (Curiosity-driven exploration)** pour sparse rewards
- **Reward Shaping** configurable (dense/sparse)
- **Curriculum Learning** de 3v1 Ã  5v5
- **AMP (Automatic Mixed Precision)** pour optimisation GPU
- **Vectorized Environments** (AsyncVectorEnv)
- **TensorBoard logging** avec mÃ©triques dÃ©taillÃ©es

## ğŸ—ï¸ Architecture

```
src/gfrl/
â”œâ”€â”€ algo/           # PPO + Storage (GAE)
â”œâ”€â”€ models/         # Policy LSTM + RND
â”œâ”€â”€ train/          # Trainer avec curriculum
â”œâ”€â”€ env/            # Wrappers GRF + Reward Shaper
â”œâ”€â”€ utils/          # Logging, encoders, schedulers
â””â”€â”€ cli/            # Entry point training
```

## ğŸ“¦ Installation

```bash
# CrÃ©er environnement virtuel
python -m venv .venv
.\.venv\Scripts\activate

# Installer dÃ©pendances
pip install -r requirements.txt
```

## ğŸš€ Training

### Sparse Rewards (avec RND)
```powershell
.\START_SPARSE.ps1
```

### Dense Rewards (reward shaping)
```powershell
python -m src.gfrl.cli.train_ppo_rnd --config configs/train_5v5.yaml
```

### Continuer training
```powershell
.\CONTINUE_SPARSE.ps1
```

## âš™ï¸ Configuration

### Train Config (`configs/train_sparse.yaml`)
```yaml
# PPO Hyperparameters
learning_rate: 0.0003
clip_epsilon: 0.2
gamma: 0.997
lambda_gae: 0.95

# RND (Curiosity)
use_rnd: true
rnd_beta: 0.05  # Intrinsic reward weight

# Architecture
hidden_size: 512
lstm_hidden_size: 128
lstm_num_layers: 1
```

### Rewards (`configs/rewards_sparse.yaml`)
```yaml
# Sparse: Goals only + minimal guidance
goal_scored: 100.0
goal_distance:
  enabled: true
  reward_scale: 0.5  # Minimal guidance
```

### Curriculum (`configs/curriculum_5v5.yaml`)
7 phases progressives:
- Phase 1-3: academy (3v1, 3v2, 3v3)
- Phase 4: 1_vs_1_easy
- Phase 5-7: 5v5 (easy â†’ medium â†’ hard)

## ğŸ“Š Monitoring

TensorBoard:
```bash
tensorboard --logdir=logs
```

Visualiser agent:
```bash
python scripts/watch_agent.py --checkpoint checkpoints/last.pt --render
```

## ğŸ”§ Fixes AppliquÃ©s

### RND Normalization
- **Bug:** Formule std incorrecte
- **Fix:** EMA du std rÃ©el du batch MSE

### LSTM Memory
- **Bug:** Mini-batch reset states
- **Fix:** Full batch pour prÃ©server mÃ©moire

### AsyncVectorEnv Info
- **Bug:** Format info non gÃ©rÃ©
- **Fix:** Extraction robuste avec 3 formats

### Hyperparameters
- `rnd_beta`: 0.5 â†’ 0.05 (task dominant)
- `entropy_coef`: 0.1 â†’ 0.02 (Ã©quilibrÃ©)
- `clip_epsilon`: 0.12 â†’ 0.2 (standard PPO)
- `learning_rate`: 0.0001 â†’ 0.0003 (standard)

## ğŸ“ˆ RÃ©sultats attendus

```
0-5M steps:    Exploration + premiers goals
5-15M steps:   Patterns Ã©mergent (10-20% winrate)
15-30M steps:  StratÃ©gie basique (25-40% winrate)
30-50M steps:  MaÃ®trise 3v1 (50-70% winrate)
50M+ steps:    Transfer vers 5v5
```

## ğŸ“š RÃ©fÃ©rences

- **PPO:** Schulman et al. (2017) - Proximal Policy Optimization
- **RND:** Burda et al. (2018) - Exploration by Random Network Distillation
- **GRF:** Kurach et al. (2020) - Google Research Football
- **GAE:** Schulman et al. (2015) - Generalized Advantage Estimation

## ğŸ“ Documentation

Voir `docs/`:
- `ARCHITECTURE_THEORY.md`: Architecture dÃ©taillÃ©e
- `RND_IMPLEMENTATION.md`: ImplÃ©mentation RND
- `SPARSE_REWARDS_PROBLEM.md`: ProblÃ¨mes sparse RL
- `INFO_FORMAT_FIXES.md`: Fixes AsyncVectorEnv

## ğŸ® Environnements supportÃ©s

- `academy_3_vs_1_with_keeper`
- `academy_3_vs_2_with_keeper`
- `academy_3_vs_3_with_keeper`
- `1_vs_1_easy`
- `5_vs_5_easy`
- `5_vs_5_medium`
- `5_vs_5_hard`

## ğŸ”¬ ExpÃ©rimentations

### Sparse vs Dense
- **Sparse + RND:** Exploration autonome, apprentissage lent mais robuste
- **Dense:** Convergence rapide, risque de sur-optimisation du shaping

### RND Beta
- `0.01`: TrÃ¨s conservateur (task >> curiositÃ©)
- `0.05`: Ã‰quilibrÃ© (recommandÃ©)
- `0.1+`: CuriositÃ© domine (agent explore sans apprendre task)

## âš ï¸ ProblÃ¨mes connus

1. **Entropy Ã©levÃ©e pendant bootstrap:** Normal avec sparse rewards
2. **Compteurs goals:** NÃ©cessite `raw_score` dans info (gÃ©rÃ©)
3. **LSTM full batch:** NÃ©cessaire pour prÃ©server mÃ©moire temporelle

## ğŸš€ Roadmap

- [ ] Self-play contre anciennes policies
- [ ] Population-based training (PBT)
- [ ] Attention mechanism dans policy
- [ ] Opponent modeling
- [ ] Multi-agent coordination

## ğŸ“„ License

MIT

## ğŸ™ Acknowledgments

- Google Research Football team
- OpenAI PPO implementation
- Burda et al. pour RND
